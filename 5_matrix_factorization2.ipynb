{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create m x n matrix for Y\n",
    "Y_train = np.loadtxt('./data/train.txt').astype(int)\n",
    "Y_test = np.loadtxt('./data/test.txt').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_err_bias(U, V, Y, A, B, reg=0.0):\n",
    "    sum_err = 0\n",
    "    for element in Y:\n",
    "        i = np.int(element[0])-1\n",
    "        j = np.int(element[1])-1\n",
    "        sum_err += (element[2]-np.dot(U[i],V[j])-A[i]-B[j])**2\n",
    "    return (reg/2 * ((np.linalg.norm(U, 'fro')+np.linalg.norm(V, 'fro'))) + sum_err)/len(Y)\n",
    "\n",
    "\n",
    "def train_model_bias(M, N, K, eta, reg, Y, eps=0.0001, max_epochs=100):\n",
    "    \"\"\"\n",
    "    Given a training data matrix Y containing rows (i, j, Y_ij)\n",
    "    where Y_ij is user i's rating on movie j, learns an\n",
    "    M x K matrix U and N x K matrix V such that rating Y_ij is approximated\n",
    "    by (UV^T)_ij.\n",
    "\n",
    "    Uses a learning rate of <eta> and regularization of <reg>. Stops after\n",
    "    <max_epochs> epochs, or once the magnitude of the decrease in regularized\n",
    "    MSE between epochs is smaller than a fraction <eps> of the decrease in\n",
    "    MSE after the first epoch.\n",
    "\n",
    "    Returns a tuple (U, V, err) consisting of U, V, and the unregularized MSE\n",
    "    of the model.\n",
    "    \"\"\"\n",
    "    #Creating U and V matrices\n",
    "    U = np.random.rand(M, K)-0.5\n",
    "    V = np.random.rand(N, K)-0.5\n",
    "    \n",
    "    #Creating A and B vectors\n",
    "    A = np.random.rand(M, 1)-0.5\n",
    "    B = np.random.rand(N, 1)-0.5\n",
    "    delta = 0\n",
    "    for epoch in range(max_epochs):\n",
    "        prev_error = get_err_bias(U, V, Y, A, B, 0)\n",
    "        \n",
    "        #Shuffling Y matrix\n",
    "        Y_shuffled = Y[np.random.permutation(np.arange(len(Y)))]\n",
    "        #Looping through the 2 dimensions of Y\n",
    "        for element in Y_shuffled:\n",
    "            i = np.int(element[0])-1\n",
    "            j = np.int(element[1])-1\n",
    "            #Computing gradient and descending along it. Trivial.\n",
    "            cur_grad_u = grad_U_bias(U[i], element[2], V[j], A[i], B[j], reg, eta)\n",
    "            cur_grad_v = grad_V_bias(V[j], element[2], U[i], A[i], B[j], reg, eta)\n",
    "            cur_grad_a = grad_A_bias(U[i], element[2], V[j], A[i], B[j], reg, eta)\n",
    "            cur_grad_b = grad_B_bias(V[j], element[2], U[i], A[i], B[j], reg, eta)\n",
    "            U[i] = U[i]-cur_grad_u\n",
    "            V[j] = V[j]-cur_grad_v\n",
    "            A[i] = A[i]-cur_grad_a\n",
    "            B[j] = B[j]-cur_grad_b\n",
    "        cur_error = get_err_bias(U, V, Y, A, B, 0)\n",
    "        \n",
    "        if (epoch==0):\n",
    "            delta = np.abs(cur_error-prev_error) # Setting loss for the first epoch.\n",
    "        elif (np.abs((cur_error-prev_error))/delta<= eps):\n",
    "            break\n",
    "    #Returning Error from final model \n",
    "    \n",
    "    return U, V, A, B, get_err_bias(U, V, Y, A, B, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 (Adding Bias Term a and b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_U_bias(Ui, Yij, Vj, Ai, Bj, reg, eta):\n",
    "    return eta * (reg * Ui.T - (Yij - (np.dot(Ui, Vj) - Ai-Bj)) * Vj.T )\n",
    "\n",
    "def grad_V_bias(Vj, Yij, Ui, Ai, Bj, reg, eta):\n",
    "    return eta * (reg*Vj.T - (Yij - np.dot(Vj,Ui) - (Ai + Bj)) * Ui.T)\n",
    "\n",
    "def grad_A_bias(Ui, Yij, Vj, Ai, Bj, reg, eta):\n",
    "    \n",
    "    return eta * (-(Yij - np.dot(Ui, Vj) - (Ai + Bj)))\n",
    "\n",
    "def grad_B_bias(Vj, Yij, Ui, Ai, Bj, reg, eta):\n",
    "    return eta * (-(Yij - np.dot(Ui, Vj) - (Ai + Bj)))\n",
    "\n",
    "def bias_err(U, V, Y, a, b, reg=0.0):    \n",
    "    err = 0\n",
    "    for x in range(Y.shape[0]):\n",
    "        i = Y[x][0] - 1\n",
    "        j = Y[x][1] - 1\n",
    "        \n",
    "        err += pow(Y[x][2] - (np.dot(U[i,:], V[j,:]) + a[i] + b[j]), 2)    \n",
    "    return 1 / Y.shape[0] * ((reg * (np.linalg.norm(U)**2 + np.linalg.norm(V)**2 \n",
    "                    + np.linalg.norm(a)**2 + np.linalg.norm(b)**2) + err) / 2) \n",
    "\n",
    "\n",
    "def train_bias_model(M, N, K, eta, reg, Y, eps=0.0001, max_epochs=300):\n",
    "    U = np.random.uniform(-0.5, 0.5, size=(M, K))\n",
    "    V = np.random.uniform(-0.5, 0.5, size=(N, K))\n",
    "    a = np.random.uniform(-0.5, 0.5, size=(M, ))\n",
    "    b = np.random.uniform(-0.5, 0.5, size=(N, ))\n",
    "    \n",
    "    mu = np.average(Y[:,2])\n",
    "    loss = []\n",
    "    loss.append(bias_err(U, V, Y, a, b, mu, reg))\n",
    "    \n",
    "    for epoch in range(max_epochs): \n",
    "        index = np.random.permutation(Y.shape[0])\n",
    "        for idx in index:\n",
    "            i = Y[idx][0] - 1\n",
    "            j = Y[idx][1] - 1\n",
    "            U[i,:] -= eta * grad_bias_U(U[i,:], Y[idx][2], V[j,:], reg, eta, a[i], b[j], mu)\n",
    "            V[j,:] -= eta * grad_bias_V(U[i,:], Y[idx][2], V[j,:], reg, eta, a[i], b[j], mu)\n",
    "            a[i] -= eta * grad_bias_a(U[i,:], Y[idx][2], V[j,:], reg, eta, a[i], b[j], mu)\n",
    "            b[j] -= eta * grad_bias_b(U[i,:], Y[idx][2], V[j,:], reg, eta, a[i], b[j], mu)\n",
    "        \n",
    "        err = bias_err(U, V, Y, a, b, mu, reg)\n",
    "        loss.append(err)\n",
    "        if (abs(loss[-1] - loss[-2]) / abs(loss[1] - loss[0]) < eps):\n",
    "            print(epoch)\n",
    "            break\n",
    "    \n",
    "    err = bias_err(U, V, Y, a, b, mu)\n",
    "    return (U, V, err, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 943 users x 1682 movies \n",
    "mu = np.average(Y_test[:,2])\n",
    "# Use to compute Ein and Eout using k=20\n",
    "U_bias, V_bias, E_in_bias, a, b = train_bias_model(M, N, k, eta, reg, Y_train)\n",
    "E_out_bias = bias_err(U, V, Y_test, a, b, mu)\n",
    "\n",
    "print(\"Training error is: \" + str(E_in_bias))\n",
    "print(\"Test error is: \" + str(E_out_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(np.transpose(U_bias), np.tranpose(V_bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3 (Using scikit-surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import train_test_split\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(pd.DataFrame(np.concatenate((Y_train, Y_test))), reader)\n",
    "\n",
    "algo = SVD(n_factors = 20, biased=True)\n",
    "\n",
    "trainset, testset = train_test_split(data, test_size=0.1)\n",
    "algo.fit(trainset)\n",
    "# User factors (u)\n",
    "u = algo.pu\n",
    "# Item factors (v)\n",
    "v = algo.qi\n",
    "predictions = algo.test(testset)\n",
    "accuracy.rmse(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

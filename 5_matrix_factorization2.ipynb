{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from surprise import Reader, Dataset\n",
    "from surprise import SVD, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 (Modifying Code from Homework 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create m x n matrix for Y\n",
    "Y_train = np.loadtxt('./data/train.txt').astype(int)\n",
    "Y_test = np.loadtxt('./data/test.txt').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_U(Ui, Yij, Vj, reg, eta):\n",
    "    return reg * Ui - Vj * (Yij - np.dot(Ui, Vj))\n",
    "\n",
    "def grad_V(Vj, Yij, Ui, reg, eta):\n",
    "    return reg * Vj - Ui * (Yij - np.dot(Ui, Vj))\n",
    "\n",
    "def get_err(U, V, Y, reg=0.0):\n",
    "    err = 0\n",
    "    for x in range(Y.shape[0]):\n",
    "        i = Y[x][0] - 1\n",
    "        j = Y[x][1] - 1\n",
    "        \n",
    "        err += pow(Y[x][2] - np.dot(U[i,:], V[j,:]), 2)    \n",
    "    return 1 / Y.shape[0] * ((reg * (np.linalg.norm(U)**2 + np.linalg.norm(V)**2) + err) / 2) \n",
    "\n",
    "def train_model(M, N, K, eta, reg, Y, eps=0.0001, max_epochs=300):\n",
    "    U = np.random.uniform(-0.5, 0.5, size=(M, K))\n",
    "    V = np.random.uniform(-0.5, 0.5, size=(N, K))\n",
    "    loss = []\n",
    "    loss.append(get_err(U, V, Y, reg))\n",
    "    \n",
    "    for epoch in range(max_epochs): \n",
    "        index = np.random.permutation(Y.shape[0])\n",
    "        for idx in index:\n",
    "            i = Y[idx][0] - 1\n",
    "            j = Y[idx][1] - 1\n",
    "            U[i,:] -= eta * grad_U(U[i,:], Y[idx][2], V[j,:], reg, eta)\n",
    "            V[j,:] -= eta * grad_V(V[j,:], Y[idx][2], U[i,:], reg, eta)\n",
    "        \n",
    "        err = get_err(U, V, Y, reg)\n",
    "        loss.append(err)\n",
    "        if (abs(loss[-1] - loss[-2]) / abs(loss[1] - loss[0]) < eps):\n",
    "            break\n",
    "    \n",
    "    err = get_err(U, V, Y)\n",
    "    return (U, V, err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factorizing with  943  users,  1682  movies.\n",
      "Training error is: 0.2494398706178021\n",
      "Test error is: 0.6733794601112535\n"
     ]
    }
   ],
   "source": [
    "# Create 943 users x 1682 movies \n",
    "M = max(max(Y_train[:,0]), max(Y_test[:,0])).astype(int) # users\n",
    "N = max(max(Y_train[:,1]), max(Y_test[:,1])).astype(int) # movies\n",
    "print(\"Factorizing with \", M, \" users, \", N, \" movies.\")\n",
    "\n",
    "k = 20\n",
    "\n",
    "reg = 0.0\n",
    "eta = 0.03 # learning rate\n",
    "\n",
    "# Use to compute Ein and Eout using k=20\n",
    "U, V, E_in = train_model(M, N, k, eta, reg, Y_train)\n",
    "E_out = get_err(U, V, Y_test)\n",
    "\n",
    "print(\"Training error is: \" + str(E_in))\n",
    "print(\"Test error is: \" + str(E_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 943)\n",
      "(2, 1682)\n"
     ]
    }
   ],
   "source": [
    "# Visualize and interpret results\n",
    "\n",
    "def visualize(U, V):\n",
    "    # The variable V is currently V^T\n",
    "    A, sigma, B = np.linalg.svd(V)\n",
    "\n",
    "    U_tilde = np.matmul(np.transpose(A[:,:2]), U)\n",
    "    V_tilde = np.matmul(np.transpose(A[:,:2]), V)\n",
    "\n",
    "    print(U_tilde.shape)\n",
    "    print(V_tilde.shape)\n",
    "    \n",
    "visualize(np.transpose(U), np.transpose(V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 (Adding Bias Term a and b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_bias_U(Ui, Yij, Vj, reg, eta, ai, bj, mu):\n",
    "    return reg * Ui - Vj * (Yij - mu - np.dot(Ui, Vj) - ai - bj)\n",
    "\n",
    "def grad_bias_V(Ui, Yij, Vj, reg, eta, ai, bj, mu):\n",
    "    return reg * Vj - Ui * (Yij - mu - np.dot(Ui, Vj) - ai - bj)\n",
    "\n",
    "def grad_bias_a(Ui, Yij, Vj, reg, eta, ai, bj, mu):\n",
    "    return reg * ai - (Yij - mu - np.dot(Ui, Vj) - ai - bj)\n",
    "\n",
    "def grad_bias_b(Ui, Yij, Vj, reg, eta, ai, bj, mu):\n",
    "    return reg * bj - (Yij - mu - np.dot(Ui, Vj) - ai - bj)\n",
    "\n",
    "def bias_err(U, V, Y, a, b, mu, reg=0.0):    \n",
    "    err = 0\n",
    "    for x in range(Y.shape[0]):\n",
    "        i = Y[x][0] - 1\n",
    "        j = Y[x][1] - 1\n",
    "        \n",
    "        err += pow(Y[x][2] - (np.dot(U[i,:], V[j,:]) + a[i] + b[j]), 2)    \n",
    "    return 1 / Y.shape[0] * ((reg * (np.linalg.norm(U)**2 + np.linalg.norm(V)**2 \n",
    "                    + np.linalg.norm(a)**2 + np.linalg.norm(b)**2) + err) / 2) \n",
    "\n",
    "def train_bias_model(M, N, K, eta, reg, Y, eps=0.0001, max_epochs=300):\n",
    "    U = np.random.uniform(-0.5, 0.5, size=(M, K))\n",
    "    V = np.random.uniform(-0.5, 0.5, size=(N, K))\n",
    "    a = np.random.uniform(-0.5, 0.5, size=(M, ))\n",
    "    b = np.random.uniform(-0.5, 0.5, size=(N, ))\n",
    "    \n",
    "    mu = np.average(Y[:,2])\n",
    "    loss = []\n",
    "    loss.append(bias_err(U, V, Y, a, b, mu, reg))\n",
    "    \n",
    "    for epoch in range(max_epochs): \n",
    "        index = np.random.permutation(Y.shape[0])\n",
    "        for idx in index:\n",
    "            i = Y[idx][0] - 1\n",
    "            j = Y[idx][1] - 1\n",
    "            U[i,:] -= eta * grad_bias_U(U[i,:], Y[idx][2], V[j,:], reg, eta, a[i], b[j], mu)\n",
    "            V[j,:] -= eta * grad_bias_V(U[i,:], Y[idx][2], V[j,:], reg, eta, a[i], b[j], mu)\n",
    "            a[i] -= eta * grad_bias_a(U[i,:], Y[idx][2], V[j,:], reg, eta, a[i], b[j], mu)\n",
    "            b[j] -= eta * grad_bias_b(U[i,:], Y[idx][2], V[j,:], reg, eta, a[i], b[j], mu)\n",
    "        \n",
    "        err = bias_err(U, V, Y, a, b, mu, reg)\n",
    "        loss.append(err)\n",
    "        if (abs(loss[-1] - loss[-2]) / abs(loss[1] - loss[0]) < eps):\n",
    "            print(epoch)\n",
    "            break\n",
    "    \n",
    "    err = bias_err(U, V, Y, a, b, mu)\n",
    "    return (U, V, err, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 943 users x 1682 movies \n",
    "mu = np.average(Y_test[:,2])\n",
    "# Use to compute Ein and Eout using k=20\n",
    "U_bias, V_bias, E_in_bias, a, b = train_bias_model(M, N, k, eta, reg, Y_train)\n",
    "E_out_bias = bias_err(U, V, Y_test, a, b, mu)\n",
    "\n",
    "print(\"Training error is: \" + str(E_in_bias))\n",
    "print(\"Test error is: \" + str(E_out_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(np.transpose(U_bias), np.tranpose(V_bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3 (Using scikit-surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import train_test_split\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(pd.DataFrame(np.concatenate((Y_train, Y_test))), reader)\n",
    "\n",
    "algo = SVD(n_factors = 20, biased=True)\n",
    "\n",
    "trainset, testset = train_test_split(data, test_size=0.1)\n",
    "algo.fit(trainset)\n",
    "# User factors (u)\n",
    "u = algo.pu\n",
    "# Item factors (v)\n",
    "v = algo.qi\n",
    "predictions = algo.test(testset)\n",
    "accuracy.rmse(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
